{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Liner Regression线性回归\n",
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Refer to [Rohan Varma](https://github.com/rohan-varma) **\n",
    "\n",
    "Linear regression is probably the first machine learning algorithm that most people learn when starting off in this field. Learning this model is a great way to get introduced to the idea of supervised learning. \n",
    "\n",
    "We have some (input, output) pairs which we denote as $ (x_i, y_i) $ and we have $n$ of these, so $i \\in [1...n]$. We want to learn a function $f: x \\rightarrow{} y$ that maps inputs to outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs and Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to learn a function $ f: x \\rightarrow{} y$ that maps information about a house to the house's price prediction. With linear regression, our function $f$ is just a ** linear combination ** of our inputs. That means our output is just the sum of our inputs, but each of our inputs are weighted by some value: \n",
    "\n",
    "$$f(x) = w_1 x_1 + w_2 x_2 + ... w_{13}x_{13} + b = \\sum_{j=1}^{13} w_j x_j + b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(dtype=tf.float32, shape=[None, 13])\n",
    "y = tf.placeholder(dtype=tf.float32, shape=[None, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions and Optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will initialize this linear model with initially random weights. As a result, our model won't be able to predict house prices very well at all. Learning is the process of adjusting these parameters so that our model's accuracy increases. In order to do this, we need to mathematically quantify how \"bad\" our model is currently. We can do this by calculating how off each prediction is from the actual value: \n",
    "\n",
    "$$ L = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - f(x_i))^2 $$\n",
    "\n",
    "If we take the derivative of this function with respect to each of the weights $w$, we will know how much to \"adjust\" each weight $w$ by in order to make our function more accurate. This is an algorithm called ** gradient descent **. \n",
    "\n",
    "If you know some multivariable calculus, you can determine that the derivative with respect to the $i$th weight is $$ \\frac{dL}{dw_i} = \\frac{-2}{N} \\sum_{i=1}^{N} (y_i - f(x_i))x_i $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.random_normal([13, 1]))\n",
    "b = tf.Variable(tf.random_normal([1]))\n",
    "y_pred = tf.matmul(x, W) + b\n",
    "loss = tf.reduce_mean(tf.square(y - y_pred))\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate = .5).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import normalize # to standardize our data\n",
    "from sklearn.model_selection import train_test_split\n",
    "data, targets = load_boston(True)\n",
    "data = normalize(data)\n",
    "targets = targets.reshape((targets.shape[0],1)) # reshape targets to follow our variables\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, targets, \n",
    "                                                    test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the part where we start training our model. We'll feeding our training set and labels into our two placeholders and then evaluate our optimizer object which minimizes the MSE loss function. We repeat this for a set number of iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Loss:', 593.89746)\n",
      "('Loss:', 64.089394)\n",
      "('Loss:', 62.071087)\n",
      "('Loss:', 60.591209)\n",
      "('Loss:', 59.312286)\n",
      "('Loss:', 58.142426)\n",
      "('Loss:', 57.049816)\n",
      "('Loss:', 56.020084)\n",
      "('Loss:', 55.045246)\n",
      "('Loss:', 54.120113)\n",
      "('Loss:', 53.240944)\n",
      "('Loss:', 52.404793)\n",
      "('Loss:', 51.609146)\n",
      "('Loss:', 50.85181)\n",
      "('Loss:', 50.130768)\n",
      "('Loss:', 49.444168)\n",
      "('Loss:', 48.790272)\n",
      "('Loss:', 48.167442)\n",
      "('Loss:', 47.574116)\n",
      "('Loss:', 47.008835)\n",
      "('Loss:', 46.470215)\n",
      "('Loss:', 45.956924)\n",
      "('Loss:', 45.467697)\n",
      "('Loss:', 45.001362)\n",
      "('Loss:', 44.556782)\n",
      "('Loss:', 44.132881)\n",
      "('Loss:', 43.728634)\n",
      "('Loss:', 43.343079)\n",
      "('Loss:', 42.975296)\n",
      "('Loss:', 42.624405)\n",
      "('Loss:', 42.289577)\n",
      "('Loss:', 41.970032)\n",
      "('Loss:', 41.665012)\n",
      "('Loss:', 41.373806)\n",
      "('Loss:', 41.095734)\n",
      "('Loss:', 40.830154)\n",
      "('Loss:', 40.576466)\n",
      "('Loss:', 40.334076)\n",
      "('Loss:', 40.102436)\n",
      "('Loss:', 39.881023)\n"
     ]
    }
   ],
   "source": [
    "numEpochs = 10000\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for i in range(numEpochs):\n",
    "    sess.run(opt, feed_dict={x: X_train, y: y_train})\n",
    "    if (i % 250 == 0):\n",
    "        print ('Loss:', loss.eval(feed_dict={x: X_train, y: y_train}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('House prices are off by an average of', 4.1875824125189522, 'thousand dollars.')\n"
     ]
    }
   ],
   "source": [
    "predictions = sess.run(y_pred, feed_dict={x: X_test})\n",
    "differences = predictions.flatten() - y_test.flatten()\n",
    "differences = [abs(x) for x in differences]\n",
    "print (\"House prices are off by an average of\", np.mean(differences), \"thousand dollars.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgQAAAFkCAYAAABfHiNRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAGTNJREFUeJzt3X+w3XV95/HnCyNJwQZmTUOkNlMsTSauu6xcCmYr4DYW\nVGZTLTuWUzKOMHalRYbJ7E5ZZnWgMnW7OA0RC6s7xR9IPB0aylBmI6hY0xqRbHMtLXpJ1xq8/ErK\nRbzJgOFH8tk/vt9bb443uff8uDnn5j4fM98h5/P9nu95+/F77nmdz/dzvt+UUpAkSfPbcf0uQJIk\n9Z+BQJIkGQgkSZKBQJIkYSCQJEkYCCRJEgYCSZKEgUCSJGEgkCRJGAgkSRJtBoIku5IcnGL5ZL1+\nYZJbkowl2Zdkc5Kls1O6JEnqlXZHCM4Clk1afh0owJ31+o3ARcDFwHnAqcBdPalUkiTNmnRzc6Mk\nG4F3lVJWJFkMPANcUkq5u16/EhgB3lJK2d6LgiVJUu91PIcgyauBS4Hb6qazgAXAAxPblFJ2AqPA\n6i5qlCRJs2xBF899D3AS8Pn68SnAS6WUvS3b7aE6vTClJK8FLgQeA/Z3UY8kSfPNIuAXgftLKc92\ns6NuAsHlwJdKKbun2S5U8wwO50JgUxd1SJI0310KfLGbHXQUCJIsB94OvHtS827g+CSLW0YJllKN\nEhzOYwB33HEHq1at6qSceWv9+vXcdNNN/S5jTrHPOmO/tc8+64z91p6RkRHWrVsH9WdpNzodIbic\n6kN+y6S2HcArwBpgYlLhCmA58OAR9rUfYNWqVZx55pkdljM/nXTSSfZZm+yzzthv7bPPOmO/dazr\nU+5tB4IkAd4PfK6UcnCivZSyN8ltwIYkzwH7gJuBbf7CQJKkwdbJCMHbgV8APjvFuvXAAWAzsBC4\nD7iy4+okSdJR0XYgKKV8BXjVYda9CFxVL5IkaY7wXgZzWKPR6HcJc4591hn7rX32WWfst/7p6kqF\nPSkgORPYsWPHDieSSJLUhuHhYYaGhgCGSinD3ezLEQJJkmQgkCRJBgJJkoSBQJIkYSCQJEkYCCRJ\nEgYCSZKEgUCSJGEgkCRJdH774577xCc+wbJly/pdxoxccMEFrFmzpt9lSJLUMwMTCL7whb9gwYLX\n9buMaR048Cyf+cztPPPM0/0uRZKknhmYQFDKWl5+eVO/y5iBj3HgwMZ+FyFJUk85h0CSJBkIJEmS\ngUCSJGEgkCRJGAgkSRIGAkmShIFAkiRhIJAkSRgIJEkSBgJJkoSBQJIkYSCQJEkYCCRJEgYCSZKE\ngUCSJGEgkCRJGAgkSRIGAkmShIFAkiTRQSBIcmqSLyQZS/JCkoeTnNmyzUeTPFWv/0qS03tXsiRJ\n6rW2AkGSk4FtwIvAhcAq4L8Az03a5hrgQ8AHgbOB54H7kxzfo5olSVKPLWhz+/8GjJZSPjCp7Qct\n21wN3FBKuRcgyfuAPcC7gTs7LVSSJM2edk8Z/Efgb5PcmWRPkuEk/xIOkpwGLAMemGgrpewFHgJW\n96JgSZLUe+0GgjcAvwvsBC4APgXcnGRdvX4ZUKhGBCbbU6+TJEkDqN1TBscB20spH6kfP5zkX1OF\nhDuO8LxQBQVJkjSA2g0ETwMjLW0jwG/W/95N9eF/CoeOEiwFvn3kXW8F1ra0NepFkqT5rdls0mw2\nD2kbHx/v2f7bDQTbgJUtbSupJxaWUnYl2Q2sAf4eIMli4BzgliPv+nxgU5vlSJI0PzQaDRqNQ78k\nDw8PMzQ01JP9txsIbgK2JbmW6hcD5wAfAH5n0jYbgQ8n+R7wGHAD8ARwT9fVSpKkWdFWICil/G2S\n9wB/BHwE2AVcXUr5s0nb3JjkBODTwMnA3wDvLKW81LuyJUlSL7U7QkApZQuwZZptrgeu76wkSZJ0\ntHkvA0mSZCCQJEkGAkmShIFAkiRhIJAkSRgIJEkSBgJJkoSBQJIkYSCQJEkYCCRJEgYCSZKEgUCS\nJGEgkCRJGAgkSRIGAkmShIFAkiRhIJAkSRgIJEkSBgJJkoSBQJIkYSCQJEkYCCRJEgYCSZKEgUCS\nJGEgkCRJGAgkSRIGAkmShIFAkiRhIJAkSRgIJEkSBgJJkoSBQJIkYSCQJEkYCCRJErCgnY2TXAdc\n19L8aCnljfX6hcAG4LeAhcD9wO+VUv65B7UOjIMHDzA8PNzvMmZsyZIlLF++vN9lSJIGWFuBoPYI\nsAZI/fiVSes2Au8ELgb2ArcAdwHndlHjgPkR4+PjDA0N9buQGVu06AR27hwxFEiSDquTQPBKKeWZ\n1sYki4HLgUtKKVvrtsuAkSRnl1K2d1fqoHgeOADcAazqcy0zMcL+/esYGxszEEiSDquTQPDLSZ4E\n9gMPAteWUh4Hhur9PTCxYSllZ5JRYDVwjASCCauAM/tdhCRJPdHupMJvAe8HLgSuAE4D/jrJicAy\n4KVSyt6W5+yp10mSpAHV1ghBKeX+SQ8fSbId+AHwXqoRg6kEKNPvfSuwtqWtUS+SJM1vzWaTZrN5\nSNv4+HjP9t/JKYN/UUoZT/KPwOnAV4HjkyxuGSVYSjVKMI3zgU3dlCNJ0jGr0WjQaBz6JXl4eLhn\nk9y7ug5BktcAvwQ8Beyg+sXBmknrVwDLqeYaSJKkAdXudQg+DtxLdZrg54E/oAoBf1ZK2ZvkNmBD\nkueAfcDNwLZj5xcGkiQdm9o9ZfB64IvAa4FngG8AbymlPFuvX0/1m7zNVBcmug+4sjelSpKk2dLu\npMIjzvArpbwIXFUvkiRpjvBeBpIkyUAgSZIMBJIkCQOBJEnCQCBJkjAQSJIkDASSJAkDgSRJwkAg\nSZIwEEiSJAwEkiQJA4EkScJAIEmSMBBIkiQMBJIkCQOBJEnCQCBJkjAQSJIkDASSJAkDgSRJwkAg\nSZIwEEiSJAwEkiQJA4EkScJAIEmSMBBIkiQMBJIkCQOBJEnCQCBJkjAQSJIkDASSJAkDgSRJwkAg\nSZLoMhAkuTbJwSQbJrUtTHJLkrEk+5JsTrK0+1IlSdJs6TgQJPkV4HeAh1tWbQQuAi4GzgNOBe7q\n9HUkSdLs6ygQJHkNcAfwAeBHk9oXA5cD60spW0sp3wYuA341ydk9qFeSJM2CTkcIbgHuLaV8raX9\nLGAB8MBEQyllJzAKrO7wtSRJ0ixb0O4TklwC/DuqD/9WpwAvlVL2trTvAZa1X54kSToa2goESV5P\nNUfg10spL7fzVKAceZOtwNqWtka9SJI0vzWbTZrN5iFt4+PjPdt/uyMEQ8DPATuSpG57FXBekg8B\n7wAWJlncMkqwlGqU4AjOBza1WY4kSfNDo9Gg0Tj0S/Lw8DBDQ0M92X+7geCrwL9pafscMAL8EfAk\n8DKwBrgbIMkKYDnwYDeFSpKk2dNWICilPA98d3JbkueBZ0spI/Xj24ANSZ4D9gE3A9tKKdt7U7Ik\nSeq1ticVTqF1bsB64ACwGVgI3Adc2YPXkSRJs6TrQFBK+bWWxy8CV9WLJEmaA7yXgSRJMhBIkiQD\ngSRJwkAgSZIwEEiSJAwEkiQJA4EkScJAIEmSMBBIkiQMBJIkCQOBJEnCQCBJkjAQSJIkDASSJAkD\ngSRJwkAgSZIwEEiSJAwEkiQJA4EkScJAIEmSMBBIkiQMBJIkCQOBJEnCQCBJkjAQSJIkDASSJAkD\ngSRJwkAgSZIwEEiSJAwEkiQJA4EkScJAIEmSMBBIkiTaDARJrkjycJLxevlmkndMWr8wyS1JxpLs\nS7I5ydLely1Jknqp3RGCx4FrgKF6+RpwT5JV9fqNwEXAxcB5wKnAXb0pVZIkzZYF7WxcSvk/LU0f\nTvK7wFuSPAlcDlxSStkKkOQyYCTJ2aWU7T2pWJIk9VzHcwiSHJfkEuAE4EGqEYMFwAMT25RSdgKj\nwOou65QkSbOorRECgCRvogoAi4B9wHtKKY8meTPwUillb8tT9gDLuq5UkiTNmrYDAfAocAZwMtVc\ngduTnHeE7QOUDl5HkiQdJW0HglLKK8D364fDSc4GrgbuBI5PsrhllGAp1SjBNLYCa1vaGvUiSdL8\n1mw2aTabh7SNj4/3bP+djBC0Og5YCOwAXgHWAHcDJFkBLKc6xTCN84FNPShHkqRjT6PRoNE49Evy\n8PAwQ0NDPdl/W4EgyR8CX6L6+eHPApdSfZJfUErZm+Q2YEOS56jmF9wMbPMXBpIkDbZ2RwhOAW4H\nXgeMA39PFQa+Vq9fDxwANlONGtwHXNmbUiVJ0mxp9zoEH5hm/YvAVfUiSZLmCO9lIEmSDASSJMlA\nIEmSMBBIkiQMBJIkCQOBJEnCQCBJkjAQSJIkDASSJAkDgSRJwkAgSZIwEEiSJAwEkiQJA4EkScJA\nIEmSMBBIkiQMBJIkCQOBJEnCQCBJkjAQSJIkDASSJAkDgSRJwkAgSZIwEEiSJAwEkiQJA4EkScJA\nIEmSMBBIkiQMBJIkCVjQ7wJ0dIyMjPS7hBlZsmQJy5cv73cZkjTvGAiOeU8Dx7Fu3bp+FzIjixad\nwM6dI4YCSTrKDATHvB8BB4E7gFV9rmU6I+zfv46xsTEDgSQdZQaCeWMVcGa/i5AkDai2JhUmuTbJ\n9iR7k+xJcneSFS3bLExyS5KxJPuSbE6ytLdlS5KkXmr3VwbnAp8EzgHeDrwa+HKSn5m0zUbgIuBi\n4DzgVOCu7kuVJEmzpa1TBqWUd01+nOT9wD8DQ8A3kiwGLgcuKaVsrbe5DBhJcnYpZXtPqpYkST3V\n7XUITgYK8MP68RBVyHhgYoNSyk5gFFjd5WtJkqRZ0nEgSBKq0wPfKKV8t25eBrxUStnbsvmeep0k\nSRpA3fzK4FbgjcBbZ7BtqEYSJEnSAOooECT5E+BdwLmllKcmrdoNHJ9kccsowVKqUYIj2AqsbWlr\n1IskSfNbs9mk2Wwe0jY+Pt6z/bcdCOow8BvA+aWU0ZbVO4BXgDXA3fX2K4DlwINH3vP5wKZ2y5Ek\naV5oNBo0God+SR4eHmZoaKgn+28rECS5leor+1rg+SSn1KvGSyn7Syl7k9wGbEjyHLAPuBnY5i8M\nJEkaXO2OEFxBNRfg6y3tlwG31/9eDxwANgMLgfuAKzsvUZIkzbZ2r0Mw7a8SSikvAlfViyRJmgO6\nvQ6BJEk6BhgIJEmSgUCSJBkIJEkSBgJJkoSBQJIkYSCQJEkYCCRJEgYCSZKEgUCSJGEgkCRJGAgk\nSRIGAkmShIFAkiRhIJAkSRgIJEkSBgJJkoSBQJIkYSCQJEkYCCRJEgYCSZKEgUCSJGEgkCRJGAgk\nSRIGAkmShIFAkiRhIJAkSRgIJEkSBgJJkoSBQJIkYSCQJEkYCCRJEgYCSZKEgUCSJNFBIEhybpK/\nTPJkkoNJ1k6xzUeTPJXkhSRfSXJ6b8qVJEmzoZMRghOBvwOuBErryiTXAB8CPgicDTwP3J/k+C7q\nlCRJs2hBu08opdwH3AeQJFNscjVwQynl3nqb9wF7gHcDd3ZeqiRJmi09nUOQ5DRgGfDARFspZS/w\nELC6l68lSZJ6p9eTCpdRnUbY09K+p14nSZIGUNunDDoUpphvcKitQOv8xEa9SJI0vzWbTZrN5iFt\n4+PjPdt/rwPBbqoP/1M4dJRgKfDtIz/1fGBTj8uRJOnY0Gg0aDQO/ZI8PDzM0NBQT/bf01MGpZRd\nVKFgzURbksXAOcA3e/lakiSpd9oeIUhyInA61UgAwBuSnAH8sJTyOLAR+HCS7wGPATcATwD39KRi\nSZLUc52cMjgL+CuqOQEF+OO6/fPA5aWUG5OcAHwaOBn4G+CdpZSXelCvJEmaBZ1ch2Ar05xqKKVc\nD1zfWUmSJOlo814GkiTJQCBJkgwEkiQJA4EkScJAIEmSMBBIkiQMBJIkCQOBJEnCQCBJkjAQSJIk\nDASSJInObm4kCRgdHWVsbKzfZczYkiVLWL58eb/LkDSgDARSB0ZHR1m5chX797/Q71JmbNGiE9i5\nc8RQIGlKBgKpA2NjY3UYuANY1e9yZmCE/fvXMTY2ZiCQNCUDgdSVVcCZ/S5CkrrmpEJJkmQgkCRJ\nBgJJkoSBQJIkYSCQJEn4KwMNoJGRkX6XMK25UKMktcNAoAHyNHAc69at63chkjTvGAg0QH4EHGRu\nXOxnC/CRfhchST1jINAAmgsX+/GUgaRji5MKJUmSgUCSJBkIJEkSBgJJkoSBQJIkYSCQJEkYCCRJ\nEgYCSZKEgWCOa/a7gDnIPutEs2m/tcs+64z91j+zdqXCJFcC/xVYBjwMXFVK+b+z9XrzUxNo9LuI\nOWZ+91mnN2X61Kc+xcqVK3tczeEtWbKE5cuXH7XX68bo6ChjY2M/1X60+2ymBr1vm80mjUb1Hj1c\n3w6iQe/XmZiVQJDkt4A/Bv4zsB1YD9yfZEUpZW78vysdU7q/cdTQ0FDvypnGokUnsHPnyMD/gR0d\nHWXlylXs3//ClOuPZp/N1LHSt4NmrvTrkczWCMF64NOllNsBklwBXARcDtw4S68p6bC6vXHUeuCm\nnlZ0eCPs37+OsbGxgf/jOjY2Vn9gTdWvR7PPZupY6dtBM3f69Uh6HgiSvBoYAj420VZKKUm+Cqzu\n9etJakenN446qcPnzRdT9at91htz4WZnx4bZGCFYArwK2NPSvgeY6oTaouo//w/437NQTq99p/7v\nFvp/x7sngE3TbLOt/u8g1Dudo1HrTPpsJuZSv0L39faq32ZiFwBbtmzpeM7D0bJr1676X1P169Hs\ns5ka/L594okn2LRp0zR9O2iqWvvRp5Nec1G3+0oppdt9HLrD5HXAk8DqUspDk9pvBN5aSvn3Ldv/\nNoP3rpEkaS65tJTyxW52MBsjBGPAAeCUlval/PSoAcD9wKXAY8D+WahHkqRj1SLgF6k+S7vS8xEC\ngCTfAh4qpVxdPw4wCtxcSvl4z19QkiR1ZbZ+ZbAB+HySHfzkZ4cnAJ+bpdeTJEldmJVAUEq5M8kS\n4KNUpw7+DriwlPLMbLyeJEnqzqycMpAkSXOL9zKQJEkGAkmSNACBIMmVSXYl+XGSbyX5lX7XNKiS\nXJfkYMvy3X7XNWiSnJvkL5M8WffR2im2+WiSp5K8kOQrSU7vR62DYro+S/LZKY69Lf2qdxAkuTbJ\n9iR7k+xJcneSFS3bLExyS5KxJPuSbE6ytF81D4IZ9tvXW461A0lu7VfN/ZbkiiQPJxmvl28mecek\n9T05zvoaCCbdBOk64M1Ud0W8v56QqKk9QjVRc1m9vLW/5QykE6kmsl4J/NQkmSTXAB8CPgicDTxP\nddwdfzSLHDBH7LPalzj02Ju/t42snAt8EjgHeDvwauDLSX5m0jYbqe7jcjFwHnAqcNdRrnPQzKTf\nCtWlayeOt9cBv3+U6xwkjwPXUN0WYAj4GnBPkombPPTmOCul9G0BvgV8YtLjUF3v8/f7WdegLlTB\nabjfdcylheqOPmtb2p4C1k96vBj4MfDeftc7CMth+uyzwF/0u7ZBXqgu236Q6oqsE8fVi8B7Jm2z\nst7m7H7XOyhLa7/VbX8FbOh3bYO8AM8Cl/XyOOvbCMGkmyA9MNFWqv8l3gTpyH65Htb9pyR3JPmF\nfhc0lyQ5jeobx+Tjbi/wEB5303lbPcT7aJJbk/yrfhc0YE6m+mb7w/rxENVPuycfazupLtLmsfYT\nrf024dIkzyT5hyQfaxlBmLeSHJfkEqpr+zxID4+z2bow0Uy0exMkVSMq7wd2Ug2hXQ/8dZI3lVKe\n72Ndc8kyqj8+Ux13y45+OXPGl6iGIHcBvwT8D2BLktV1kJ/X6quxbgS+UUqZmNezDHipDpyTeazV\nDtNvUN3f5gdUo3n/FrgRWAH8p6Ne5IBI8iaqALAI2Ec1IvBokjfTo+Osn4HgcMLhz2HOa6WUydeq\nfiTJdqo3zXuphnTVOY+7Iyil3Dnp4XeS/APwT8DbqIZ357tbgTcyszk9Hms/MdFvvzq5sZTyp5Me\nfifJbuCrSU4rpexifnoUOINqROVi4PYk5x1h+7aPs35OKmz3JkhqUUoZB/4RmNcz5Nu0m+qN4nHX\nhfqP8hgeeyT5E+BdwNtKKU9NWrUbOD7J4paneKzxU/329DSbP0T1vp23x1sp5ZVSyvdLKcOllP9O\nNQn/anp4nPUtEJRSXgZ2AGsm2urhozXAN/tV11yS5DVUw7fTvZlUqz/IdnPocbeYasazx90MJXk9\n8Frm+bFXf6j9BvAfSimjLat3AK9w6LG2AlhONfQ7b03Tb1N5M9W33Xl9vLU4DlhID4+zfp8y8CZI\nbUjyceBeqtMEPw/8AdWB0OxnXYMmyYlU3yRSN70hyRnAD0spj1Ods/xwku9R3Xb7Bqpft9zTh3IH\nwpH6rF6uo5pDsLve7n9SjU51fcvVuar+XXwDWAs8n2Ri1Gm8lLK/lLI3yW3AhiTPUZ33vRnYVkrZ\n3p+q+2+6fkvyBuC3gS1UM+nPoPqs2FpKeaQfNfdbkj+kmsfzOPCzwKXA+cAFPT3OBuCnE79H9Uf5\nx1Rp5qx+1zSoC9UH/xN1X40CXwRO63ddg7bUb5SDVKekJi+fmbTN9VQTll6g+lA7vd91D2qfUU1i\nuo8qDOwHvg/8L+Dn+l13n/tsqv46ALxv0jYLqX5zP1b/of5zYGm/ax/kfgNeD3wdeKZ+f+6kmsT6\nmn7X3sc++9P6fffj+n34ZeDXen2ceXMjSZLU/0sXS5Kk/jMQSJIkA4EkSTIQSJIkDASSJAkDgSRJ\nwkAgSZIwEEiSJAwEkiQJA4EkScJAIEmSgP8P1LTZVG+RjmEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f236a8b0ad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist(differences)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Logistic Regression with MNIST （逻辑回归）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll see how this regression model can perform on the MNIST dataset. Now, as you know, linear regression is a model that produces continuous values. Like we saw in the last example, the model predicted house prices which can take any real value. However, there are prediction tasks where we'd like the model to output a category or class. In the case of binary classification, you'd want the network to output the probabilities for the input being part of either class 1 or class 2.\n",
    "\n",
    "This leads us to another type of regression called logistic regression. You can think of logistic regression as being the same thing as linear regression, except it outputs probabilities instead of real values. The way the model acheives this is by placing the output of (wx + b) through a softmax layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./../mnist/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ./../mnist/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./../mnist/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./../mnist/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "step 0, training accuracy 0.109375\n",
      "step 1000, training accuracy 0.59375\n",
      "step 2000, training accuracy 0.609375\n",
      "step 3000, training accuracy 0.8125\n",
      "step 4000, training accuracy 0.828125\n",
      "step 5000, training accuracy 0.828125\n",
      "step 6000, training accuracy 0.921875\n",
      "step 7000, training accuracy 0.890625\n",
      "step 8000, training accuracy 0.859375\n",
      "step 9000, training accuracy 0.828125\n",
      "step 10000, training accuracy 0.890625\n",
      "step 11000, training accuracy 0.828125\n",
      "step 12000, training accuracy 0.890625\n",
      "step 13000, training accuracy 0.890625\n",
      "step 14000, training accuracy 0.8125\n",
      "step 15000, training accuracy 0.90625\n",
      "step 16000, training accuracy 0.90625\n",
      "step 17000, training accuracy 0.90625\n",
      "step 18000, training accuracy 0.90625\n",
      "step 19000, training accuracy 0.953125\n",
      "test accuracy 0.921875\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"./../mnist/MNIST_data/\", one_hot=True)\n",
    "\n",
    "numClasses = 10\n",
    "inputSize = 784  \n",
    "trainingIterations = 20000\n",
    "batchSize = 64\n",
    "tf.reset_default_graph() \n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None, inputSize])\n",
    "y = tf.placeholder(tf.float32, shape = [None, numClasses])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([inputSize, numClasses], stddev=0.1))\n",
    "B1 = tf.Variable(tf.constant(0.1), [numClasses])\n",
    "\n",
    "y_pred = tf.nn.softmax(tf.matmul(X, W1) + B1)\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(y - y_pred))\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate = .05).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(trainingIterations):\n",
    "    batch = mnist.train.next_batch(batchSize)\n",
    "    batchInput = batch[0]\n",
    "    batchLabels = batch[1]\n",
    "    _, trainingLoss = sess.run([opt, loss], feed_dict={X: batchInput, y: batchLabels})\n",
    "    if i%1000 == 0:\n",
    "        train_accuracy = accuracy.eval(session=sess, feed_dict={X: batchInput, y: batchLabels})\n",
    "        print (\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "\n",
    "batch = mnist.test.next_batch(batchSize)\n",
    "testAccuracy = sess.run(accuracy, feed_dict={X: batch[0], y: batch[1]})\n",
    "print (\"test accuracy %g\"%(testAccuracy))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
