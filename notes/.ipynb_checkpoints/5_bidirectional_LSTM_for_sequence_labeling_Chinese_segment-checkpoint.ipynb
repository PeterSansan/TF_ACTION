{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于 Bi-directional LSTM 的序列标注任务（分词）\n",
    "转载自：https://github.com/yongyehuang/Tensorflow-Tutorial <br/>\n",
    "本例子主要参考[【中文分词系列】 4. 基于双向LSTM的seq2seq字标注]{url: http://spaces.ac.cn/archives/3924/} 这篇文章。<br/>\n",
    "该文章用的是 keras 实现的双端 LSTM，在本例中，实现思路和该文章基本上一样，只是用 TensorFlow 来实现的。<br/>\n",
    "本例最主要的是说明基于 TensorFlow 如何来实现双端 LSTM。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@author: huangyongye <br/>\n",
    "@creat_date: 2017-04-19 <br/>\n",
    "reference: <br/>\n",
    "[1] 【中文分词系列】 4. 基于双向LSTM的seq2seq字标注 http://spaces.ac.cn/archives/3924/  <br/>\n",
    "[2] https://github.com/yongyehuang/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/bidirectional_rnn.py  <br/>\n",
    "[3] https://github.com/yongyehuang/deepnlp/blob/master/deepnlp/pos/pos_model_bilstm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of texts is 20247877\n",
      "Example of texts: \n",
      " 人/b  们/e  常/s  说/s  生/b  活/e  是/s  一/s  部/s  教/b  科/m  书/e  ，/s  而/s  血/s  与/s  火/s  的/s  战/b  争/e  更/s  是/s  不/b  可/m  多/m  得/e  的/s  教/b  科/m  书/e  ，/s  她/s  确/b  实/e  是/s  名/b  副/m  其/m  实/e  的/s  ‘/s  我/s  的/s  大/b  学/e  ’/s  。/s   心/s  静/s  渐/s  知/s  春/s  似/s  海/s  ，/s  花/s  深/s  每/s  觉/s  影/s\n"
     ]
    }
   ],
   "source": [
    "# 以字符串的形式读入所有数据\n",
    "with open('Data/msr_train.txt', 'rb') as inp:\n",
    "    texts = inp.read().decode('gbk')\n",
    "# 根据换行切分\n",
    "sentences = texts.split('\\r\\n')  # 根据换行切分成\n",
    "\n",
    "# 将不规范的内容（如每行的开头）去掉\n",
    "def clean(s): \n",
    "    if u'“/s' not in s:  # 句子中间的引号不应去掉\n",
    "        return s.replace(u' ”/s', '')\n",
    "    elif u'”/s' not in s:\n",
    "        return s.replace(u'“/s ', '')\n",
    "    elif u'‘/s' not in s:\n",
    "        return s.replace(u' ’/s', '')\n",
    "    elif u'’/s' not in s:\n",
    "        return s.replace(u'‘/s ', '')\n",
    "    else:\n",
    "        return s\n",
    "    \n",
    "texts = u''.join(map(clean, sentences)) # 把所有的词拼接起来\n",
    "print 'Length of texts is %d' % len(texts)\n",
    "print 'Example of texts: \\n', texts[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences number: 331739\n",
      "Sentence Example:\n",
      " 人/b  们/e  常/s  说/s  生/b  活/e  是/s  一/s  部/s  教/b  科/m  书/e  \n"
     ]
    }
   ],
   "source": [
    "# 重新以标点来划分\n",
    "sentences = re.split(u'[，。！？、‘’“”]/[bems]', texts)\n",
    "print 'Sentences number:', len(sentences)\n",
    "print 'Sentence Example:\\n', sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12285it [00:00, 122840.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start creating words and tags data ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "331739it [00:02, 119316.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of datas is 321533\n",
      "Example of datas:  [u'\\u4eba' u'\\u4eec' u'\\u5e38' u'\\u8bf4' u'\\u751f' u'\\u6d3b' u'\\u662f'\n",
      " u'\\u4e00' u'\\u90e8' u'\\u6559' u'\\u79d1' u'\\u4e66']\n",
      "Example of labels: [u'b' u'e' u's' u's' u'b' u'e' u's' u's' u's' u'b' u'm' u'e']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_Xy(sentence):\n",
    "    \"\"\"将 sentence 处理成 [word1, w2, ..wn], [tag1, t2, ...tn]\"\"\"\n",
    "    words_tags = re.findall('(.)/(.)', sentence)\n",
    "    if words_tags:\n",
    "        words_tags = np.asarray(words_tags)\n",
    "        words = words_tags[:, 0]\n",
    "        tags = words_tags[:, 1]\n",
    "        return words, tags # 所有的字和tag分别存为 data / label\n",
    "    return None\n",
    "\n",
    "datas = list()\n",
    "labels = list()\n",
    "print 'Start creating words and tags data ...'\n",
    "for sentence in tqdm(iter(sentences)):\n",
    "    result = get_Xy(sentence)\n",
    "    if result:\n",
    "        datas.append(result[0])\n",
    "        labels.append(result[1])\n",
    "\n",
    "print 'Length of datas is %d' % len(datas) \n",
    "print 'Example of datas: ', datas[0]\n",
    "print 'Example of labels:', labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tags</th>\n",
       "      <th>words</th>\n",
       "      <th>sentence_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[b, e, s, s, b, e, s, s, s, b, m, e]</td>\n",
       "      <td>[人, 们, 常, 说, 生, 活, 是, 一, 部, 教, 科, 书]</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[s, s, s, s, s, b, e, s, s, b, m, m, e, s, b, ...</td>\n",
       "      <td>[而, 血, 与, 火, 的, 战, 争, 更, 是, 不, 可, 多, 得, 的, 教, ...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tags  \\\n",
       "0               [b, e, s, s, b, e, s, s, s, b, m, e]   \n",
       "1  [s, s, s, s, s, b, e, s, s, b, m, m, e, s, b, ...   \n",
       "\n",
       "                                               words  sentence_len  \n",
       "0               [人, 们, 常, 说, 生, 活, 是, 一, 部, 教, 科, 书]            12  \n",
       "1  [而, 血, 与, 火, 的, 战, 争, 更, 是, 不, 可, 多, 得, 的, 教, ...            17  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pd.DataFrame({'words': datas, 'tags': labels}, index=range(len(datas)))\n",
    "#　句子长度\n",
    "df_data['sentence_len'] = df_data['words'].apply(lambda words: len(words))\n",
    "df_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkEAAAGHCAYAAAC3XYaZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npP8DCHzx/GXd0QAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff3b44d4450>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 句子长度的分布\n",
    "import matplotlib.pyplot as plt\n",
    "df_data['sentence_len'].hist(bins=100)\n",
    "plt.xlim(0, 100)\n",
    "plt.xlabel('sentence_length')\n",
    "plt.ylabel('sentence_num')\n",
    "plt.title('Distribution of the Length of Sentence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上图可以看到，在使用标点进行分割后，绝大部分的句子长度小于30个字。因为一般情况下，我们训练网络的时候都喜欢把输入 padding 到固定的长度，这样子计算更快。因此我们取 32 作为句子长度，超过 32 个字的将把多余的字去掉，少于 32 个字的将用特殊字符填充。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1.用 chain(*lists) 函数把多个list拼接起来\n",
    "from itertools import chain\n",
    "all_words = list(chain(*df_data['words'].values))\n",
    "# 2.统计所有 word\n",
    "sr_allwords = pd.Series(all_words)\n",
    "sr_allwords = sr_allwords.value_counts()\n",
    "set_words = sr_allwords.index\n",
    "set_ids = range(1, len(set_words)+1) # 注意从1开始，因为我们准备把0作为填充值\n",
    "tags = [ 'x', 's', 'b', 'm', 'e']\n",
    "tag_ids = range(len(tags))\n",
    "\n",
    "# 3. 构建 words 和 tags 都转为数值 id 的映射（使用 Series 比 dict 更加方便）\n",
    "word2id = pd.Series(set_ids, index=set_words)\n",
    "id2word = pd.Series(set_words, index=set_ids)\n",
    "tag2id = pd.Series(tag_ids, index=tags)\n",
    "id2tag = pd.Series(tags, index=tag_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size=5158\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(set_words)\n",
    "print 'vocab_size={}'.format(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把 words 和 tags 都转为数值 id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 31s, sys: 364 ms, total: 1min 32s\n",
      "Wall time: 1min 31s\n"
     ]
    }
   ],
   "source": [
    "max_len = 32\n",
    "def X_padding(words):\n",
    "    \"\"\"把 words 转为 id 形式，并自动补全位 max_len 长度。\"\"\"\n",
    "    ids = list(word2id[words])\n",
    "    if len(ids) >= max_len:  # 长则弃掉\n",
    "        return ids[:max_len]\n",
    "    ids.extend([0]*(max_len-len(ids))) # 短则补全\n",
    "    return ids\n",
    "\n",
    "def y_padding(tags):\n",
    "    \"\"\"把 tags 转为 id 形式， 并自动补全位 max_len 长度。\"\"\"\n",
    "    ids = list(tag2id[tags])\n",
    "    if len(ids) >= max_len:  # 长则弃掉\n",
    "        return ids[:max_len]\n",
    "    ids.extend([0]*(max_len-len(ids))) # 短则补全\n",
    "    return ids\n",
    "\n",
    "%time df_data['X'] = df_data['words'].apply(X_padding)\n",
    "%time df_data['y'] = df_data['tags'].apply(y_padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 最后得到了所有的数据\n",
    "X = np.asarray(list(df_data['X'].values))\n",
    "y = np.asarray(list(df_data['y'].values))\n",
    "print 'X.shape={}, y.shape={}'.format(X.shape, y.shape)\n",
    "print 'Example of words: ', df_data['words'].values[0]\n",
    "print 'Example of X: ', X[0]\n",
    "print 'Example of tags: ', df_data['tags'].values[0]\n",
    "print 'Example of y: ', y[0]\n",
    "\n",
    "# 保存数据\n",
    "import pickle\n",
    "with open('create_data/data.pkl', 'wb') as outp:\n",
    "    %time pickle.dump(X, outp)\n",
    "    %time pickle.dump(y, outp)\n",
    "    pickle.dump(word2id, outp)\n",
    "    pickle.dump(id2word, outp)\n",
    "    pickle.dump(tag2id, outp)\n",
    "    pickle.dump(id2tag, outp)\n",
    "print '** Finished saving the data.'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 导入数据\n",
    "import pickle\n",
    "with open('create_data/data.pkl', 'rb') as inp:\n",
    "    %time X = pickle.load(inp)\n",
    "    %time y = pickle.load(inp)\n",
    "    word2id = pickle.load(inp)\n",
    "    id2word = pickle.load(inp)\n",
    "    tag2id = pickle.load(inp)\n",
    "    id2tag = pickle.load(inp)\n",
    "\n",
    "# 划分测试集/训练集/验证集\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train,  test_size=0.2, random_state=42)\n",
    "print 'X_train.shape={}, y_train.shape={}; \\nX_valid.shape={}, y_valid.shape={};\\nX_test.shape={}, y_test.shape={}'.format(\n",
    "    X_train.shape, y_train.shape, X_valid.shape, y_valid.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 构造一个生成batch数据的类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ** 3.build the data generator\n",
    "class BatchGenerator(object):\n",
    "    \"\"\" Construct a Data generator. The input X, y should be ndarray or list like type.\n",
    "    \n",
    "    Example:\n",
    "        Data_train = BatchGenerator(X=X_train_all, y=y_train_all, shuffle=False)\n",
    "        Data_test = BatchGenerator(X=X_test_all, y=y_test_all, shuffle=False)\n",
    "        X = Data_train.X\n",
    "        y = Data_train.y\n",
    "        or:\n",
    "        X_batch, y_batch = Data_train.next_batch(batch_size)\n",
    "     \"\"\" \n",
    "    \n",
    "    def __init__(self, X, y, shuffle=False):\n",
    "        if type(X) != np.ndarray:\n",
    "            X = np.asarray(X)\n",
    "        if type(y) != np.ndarray:\n",
    "            y = np.asarray(y)\n",
    "        self._X = X\n",
    "        self._y = y\n",
    "        self._epochs_completed = 0\n",
    "        self._index_in_epoch = 0\n",
    "        self._number_examples = self._X.shape[0]\n",
    "        self._shuffle = shuffle\n",
    "        if self._shuffle:\n",
    "            new_index = np.random.permutation(self._number_examples)\n",
    "            self._X = self._X[new_index]\n",
    "            self._y = self._y[new_index]\n",
    "                \n",
    "    @property\n",
    "    def X(self):\n",
    "        return self._X\n",
    "    \n",
    "    @property\n",
    "    def y(self):\n",
    "        return self._y\n",
    "    \n",
    "    @property\n",
    "    def num_examples(self):\n",
    "        return self._number_examples\n",
    "    \n",
    "    @property\n",
    "    def epochs_completed(self):\n",
    "        return self._epochs_completed\n",
    "    \n",
    "    def next_batch(self, batch_size):\n",
    "        \"\"\" Return the next 'batch_size' examples from this data set.\"\"\"\n",
    "        start = self._index_in_epoch\n",
    "        self._index_in_epoch += batch_size\n",
    "        if self._index_in_epoch > self._number_examples:\n",
    "            # finished epoch\n",
    "            self._epochs_completed += 1\n",
    "            # Shuffle the data \n",
    "            if self._shuffle:\n",
    "                new_index = np.random.permutation(self._number_examples)\n",
    "                self._X = self._X[new_index]\n",
    "                self._y = self._y[new_index]\n",
    "            start = 0\n",
    "            self._index_in_epoch = batch_size\n",
    "            assert batch_size <= self._number_examples\n",
    "        end = self._index_in_epoch\n",
    "        return self._X[start:end], self._y[start:end]\n",
    "\n",
    "print 'Creating the data generator ...'\n",
    "data_train = BatchGenerator(X_train, y_train, shuffle=True)\n",
    "data_valid = BatchGenerator(X_valid, y_valid, shuffle=False)\n",
    "data_test = BatchGenerator(X_test, y_test, shuffle=False)\n",
    "print 'Finished creating the data generator.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bi-directional lstm 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 模型构造"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "\n",
    "'''\n",
    "For Chinese word segmentation.\n",
    "'''\n",
    "# ##################### config ######################\n",
    "decay = 0.85\n",
    "max_epoch = 5\n",
    "max_max_epoch = 10\n",
    "timestep_size = max_len = 32           # 句子长度\n",
    "vocab_size = 5159    # 样本中不同字的个数+1(padding 0)，根据处理数据的时候得到\n",
    "input_size = embedding_size = 64       # 字向量长度\n",
    "class_num = 5\n",
    "hidden_size = 128    # 隐含层节点数\n",
    "layer_num = 2        # bi-lstm 层数\n",
    "max_grad_norm = 5.0  # 最大梯度（超过此值的梯度将被裁剪）\n",
    "\n",
    "lr = tf.placeholder(tf.float32)\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "batch_size = tf.placeholder(tf.int32)  # 注意类型必须为 tf.int32\n",
    "model_save_path = 'ckpt/bi-lstm.ckpt'  # 模型保存位置\n",
    "\n",
    "\n",
    "def weight_variable(shape):\n",
    "    \"\"\"Create a weight variable with appropriate initialization.\"\"\"\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "X_inputs = tf.placeholder(tf.int32, [None, timestep_size], name='X_input')\n",
    "y_inputs = tf.placeholder(tf.int32, [None, timestep_size], name='y_input')    \n",
    "\n",
    "\n",
    "def bi_lstm(X_inputs):\n",
    "    \"\"\"build the bi-LSTMs network. Return the y_pred\"\"\"\n",
    "    # ** 0.char embedding\n",
    "    embedding = tf.get_variable(\"embedding\", [vocab_size, embedding_size], dtype=tf.float32)\n",
    "    # X_inputs.shape = [batchsize, timestep_size]  ->  inputs.shape = [batchsize, timestep_size, embedding_size]\n",
    "    inputs = tf.nn.embedding_lookup(embedding, X_inputs)  \n",
    "    # ** 1.LSTM 层\n",
    "    lstm_fw_cell = rnn.BasicLSTMCell(hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_bw_cell = rnn.BasicLSTMCell(hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
    "    # ** 2.dropout\n",
    "    lstm_fw_cell = rnn.DropoutWrapper(cell=lstm_fw_cell, input_keep_prob=1.0, output_keep_prob=keep_prob)\n",
    "    lstm_bw_cell = rnn.DropoutWrapper(cell=lstm_bw_cell, input_keep_prob=1.0, output_keep_prob=keep_prob)\n",
    "    # ** 3.多层 LSTM\n",
    "    cell_fw = rnn.MultiRNNCell([lstm_fw_cell]*layer_num, state_is_tuple=True)\n",
    "    cell_bw = rnn.MultiRNNCell([lstm_bw_cell]*layer_num, state_is_tuple=True)\n",
    "    # ** 4.初始状态\n",
    "    initial_state_fw = cell_fw.zero_state(batch_size, tf.float32)\n",
    "    initial_state_bw = cell_bw.zero_state(batch_size, tf.float32)  \n",
    "    \n",
    "    # 下面两部分是等价的\n",
    "    # **************************************************************\n",
    "    # ** 把 inputs 处理成 rnn.static_bidirectional_rnn 的要求形式\n",
    "    # ** 文档说明\n",
    "    # inputs: A length T list of inputs, each a tensor of shape\n",
    "    # [batch_size, input_size], or a nested tuple of such elements.\n",
    "    # *************************************************************\n",
    "    # Unstack to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    # inputs.shape = [batchsize, timestep_size, embedding_size]  ->  timestep_size tensor, each_tensor.shape = [batchsize, embedding_size]\n",
    "    # inputs = tf.unstack(inputs, timestep_size, 1)\n",
    "    # ** 5.bi-lstm 计算（tf封装）  一般采用下面 static_bidirectional_rnn 函数调用。\n",
    "    #   但是为了理解计算的细节，所以把后面的这段代码进行展开自己实现了一遍。\n",
    "#     try:\n",
    "#         outputs, _, _ = rnn.static_bidirectional_rnn(cell_fw, cell_bw, inputs, \n",
    "#                         initial_state_fw = initial_state_fw, initial_state_bw = initial_state_bw, dtype=tf.float32)\n",
    "#     except Exception: # Old TensorFlow version only returns outputs not states\n",
    "#         outputs = rnn.static_bidirectional_rnn(cell_fw, cell_bw, inputs, \n",
    "#                         initial_state_fw = initial_state_fw, initial_state_bw = initial_state_bw, dtype=tf.float32)\n",
    "#     output = tf.reshape(tf.concat(outputs, 1), [-1, hidden_size * 2])\n",
    "    # ***********************************************************\n",
    "    \n",
    "    # ***********************************************************\n",
    "    # ** 5. bi-lstm 计算（展开）\n",
    "    with tf.variable_scope('bidirectional_rnn'):\n",
    "        # *** 下面，两个网络是分别计算 output 和 state \n",
    "        # Forward direction\n",
    "        outputs_fw = list()\n",
    "        state_fw = initial_state_fw\n",
    "        with tf.variable_scope('fw'):\n",
    "            for timestep in range(timestep_size):\n",
    "                if timestep > 0:\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "                (output_fw, state_fw) = cell_fw(inputs[:, timestep, :], state_fw)\n",
    "                outputs_fw.append(output_fw)\n",
    "        \n",
    "        # backward direction\n",
    "        outputs_bw = list()\n",
    "        state_bw = initial_state_bw\n",
    "        with tf.variable_scope('bw') as bw_scope:\n",
    "            inputs = tf.reverse(inputs, [1])\n",
    "            for timestep in range(timestep_size):\n",
    "                if timestep > 0:\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "                (output_bw, state_bw) = cell_bw(inputs[:, timestep, :], state_bw)\n",
    "                outputs_bw.append(output_bw)\n",
    "        # *** 然后把 output_bw 在 timestep 维度进行翻转\n",
    "        # outputs_bw.shape = [timestep_size, batch_size, hidden_size]\n",
    "        outputs_bw = tf.reverse(outputs_bw, [0])\n",
    "        # 把两个oupputs 拼成 [timestep_size, batch_size, hidden_size*2]\n",
    "        output = tf.concat([outputs_fw, outputs_bw], 2)\n",
    "        output = tf.transpose(output, perm=[1,0,2])\n",
    "        output = tf.reshape(output, [-1, hidden_size*2])\n",
    "    # ***********************************************************\n",
    "    \n",
    "    softmax_w = weight_variable([hidden_size * 2, class_num]) \n",
    "    softmax_b = bias_variable([class_num]) \n",
    "    logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "    return logits\n",
    "\n",
    "\n",
    "y_pred = bi_lstm(X_inputs)\n",
    "# adding extra statistics to monitor\n",
    "# y_inputs.shape = [batch_size, timestep_size]\n",
    "correct_prediction = tf.equal(tf.cast(tf.argmax(y_pred, 1), tf.int32), tf.reshape(y_inputs, [-1]))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels = tf.reshape(y_inputs, [-1]), logits = y_pred))\n",
    "\n",
    "# ***** 优化求解 *******\n",
    "# 获取模型的所有参数\n",
    "tvars = tf.trainable_variables()\n",
    "# 获取损失函数对于每个参数的梯度\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), max_grad_norm)\n",
    "# 优化器\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "# 梯度下降计算\n",
    "train_op = optimizer.apply_gradients( zip(grads, tvars),\n",
    "    global_step=tf.contrib.framework.get_or_create_global_step())\n",
    "print 'Finished creating the bi-lstm model.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_epoch(dataset):\n",
    "    \"\"\"Testing or valid.\"\"\"\n",
    "    _batch_size = 5000\n",
    "    fetches = [accuracy, cost]\n",
    "    _y = dataset.y\n",
    "    data_size = _y.shape[0]\n",
    "    batch_num = int(data_size / _batch_size)\n",
    "    start_time = time.time()\n",
    "    _costs = 0.0\n",
    "    _accs = 0.0\n",
    "    for i in xrange(batch_num):\n",
    "        X_batch, y_batch = dataset.next_batch(_batch_size)\n",
    "        feed_dict = {X_inputs:X_batch, y_inputs:y_batch, lr:1e-5, batch_size:_batch_size, keep_prob:1.0}\n",
    "        _acc, _cost = sess.run(fetches, feed_dict)\n",
    "        _accs += _acc\n",
    "        _costs += _cost    \n",
    "    mean_acc= _accs / batch_num     \n",
    "    mean_cost = _costs / batch_num\n",
    "    return mean_acc, mean_cost\n",
    "\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "tr_batch_size = 128 \n",
    "max_max_epoch = 12\n",
    "display_num = 5  # 每个 epoch 显示是个结果\n",
    "tr_batch_num = int(data_train.y.shape[0] / tr_batch_size)  # 每个 epoch 中包含的 batch 数\n",
    "display_batch = int(tr_batch_num / display_num)  # 每训练 display_batch 之后输出一次\n",
    "saver = tf.train.Saver(max_to_keep=10)  # 最多保存的模型数量\n",
    "for epoch in xrange(max_max_epoch):\n",
    "    _lr = 1e-4\n",
    "    if epoch > max_epoch:\n",
    "        _lr = _lr * ((decay) ** (epoch - max_epoch))\n",
    "    print 'EPOCH %d， lr=%g' % (epoch+1, _lr)\n",
    "    start_time = time.time()\n",
    "    _costs = 0.0\n",
    "    _accs = 0.0\n",
    "    show_accs = 0.0\n",
    "    show_costs = 0.0\n",
    "    for batch in xrange(tr_batch_num): \n",
    "        fetches = [accuracy, cost, train_op]\n",
    "        X_batch, y_batch = data_train.next_batch(tr_batch_size)\n",
    "        feed_dict = {X_inputs:X_batch, y_inputs:y_batch, lr:_lr, batch_size:tr_batch_size, keep_prob:0.5}\n",
    "        _acc, _cost, _ = sess.run(fetches, feed_dict) # the cost is the mean cost of one batch\n",
    "        _accs += _acc\n",
    "        _costs += _cost\n",
    "        show_accs += _acc\n",
    "        show_costs += _cost\n",
    "        if (batch + 1) % display_batch == 0:\n",
    "            valid_acc, valid_cost = test_epoch(data_valid)  # valid\n",
    "            print '\\ttraining acc=%g, cost=%g;  valid acc= %g, cost=%g ' % (show_accs / display_batch,\n",
    "                                                show_costs / display_batch, valid_acc, valid_cost)\n",
    "            show_accs = 0.0\n",
    "            show_costs = 0.0\n",
    "    mean_acc = _accs / tr_batch_num \n",
    "    mean_cost = _costs / tr_batch_num\n",
    "    if (epoch + 1) % 3 == 0:  # 每 3 个 epoch 保存一次模型\n",
    "        save_path = saver.save(sess, model_save_path, global_step=(epoch+1))\n",
    "        print 'the save path is ', save_path\n",
    "    print '\\ttraining %d, acc=%g, cost=%g ' % (data_train.y.shape[0], mean_acc, mean_cost)\n",
    "    print 'Epoch training %d, acc=%g, cost=%g, speed=%g s/epoch' % (data_train.y.shape[0], mean_acc, mean_cost, time.time()-start_time)        \n",
    "# testing\n",
    "print '**TEST RESULT:'\n",
    "test_acc, test_cost = test_epoch(data_test)\n",
    "print '**Test %d, acc=%g, cost=%g' % (data_test.y.shape[0], test_acc, test_cost) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型测试，现在给定一个字符串，首先应该把处理成正确的模型输入形式。即每次输入一个片段，（这里限制了每个片段的长度不超过 max_len=32）。每个字处理为对应的 id， 每个片段都会 padding 处理到固定的长度。也就是说，输入的是一个list， list 的每个元素是一个包含多个 id 的list。<br/>\n",
    "即 [[id0, id1, ..., id31], [id0, id1, ..., id31], [], ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ** 导入模型\n",
    "saver = tf.train.Saver()\n",
    "best_model_path = 'ckpt/bi-lstm.ckpt-12'\n",
    "%time saver.restore(sess, best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 再看看模型的输入数据形式, 我们要进行分词，首先就要把句子转为这样的形式\n",
    "X_tt, y_tt = data_train.next_batch(2)\n",
    "print 'X_tt.shape=', X_tt.shape, 'y_tt.shape=', y_tt.shape\n",
    "print 'X_tt = ', X_tt\n",
    "print 'y_tt = ', y_tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#转移概率，单纯用了等概率\n",
    "zy = {'be':0.5,\n",
    "      'bm':0.5,\n",
    "      'eb':0.5,\n",
    "      'es':0.5,\n",
    "      'me':0.5,\n",
    "      'mm':0.5,\n",
    "      'sb':0.5,\n",
    "      'ss':0.5\n",
    "     }\n",
    "zy = {i:np.log(zy[i]) for i in zy.keys()}\n",
    "\n",
    "\n",
    "def viterbi(nodes):\n",
    "    \"\"\"\n",
    "    维特比译码：除了第一层以外，每一层有4个节点。\n",
    "    计算当前层（第一层不需要计算）四个节点的最短路径：\n",
    "       对于本层的每一个节点，计算出路径来自上一层的各个节点的新的路径长度（概率）。保留最大值（最短路径）。\n",
    "       上一层每个节点的路径保存在 paths 中。计算本层的时候，先用paths_ 暂存，然后把本层的最大路径保存到 paths 中。\n",
    "       paths 采用字典的形式保存（路径：路径长度）。\n",
    "       一直计算到最后一层，得到四条路径，将长度最短（概率值最大的路径返回）\n",
    "    \"\"\"\n",
    "    paths = {'b': nodes[0]['b'], 's':nodes[0]['s']} # 第一层，只有两个节点\n",
    "    for layer in xrange(1, len(nodes)):  # 后面的每一层\n",
    "        paths_ = paths.copy()  # 先保存上一层的路径\n",
    "        # node_now 为本层节点， node_last 为上层节点\n",
    "        paths = {}  # 清空 path \n",
    "        for node_now in nodes[layer].keys():\n",
    "            # 对于本层的每个节点，找出最短路径\n",
    "            sub_paths = {} \n",
    "            # 上一层的每个节点到本层节点的连接\n",
    "            for path_last in paths_.keys():\n",
    "                if path_last[-1] + node_now in zy.keys(): # 若转移概率不为 0 \n",
    "                    sub_paths[path_last + node_now] = paths_[path_last] + nodes[layer][node_now] + zy[path_last[-1] + node_now]\n",
    "            # 最短路径,即概率最大的那个\n",
    "            sr_subpaths = pd.Series(sub_paths)\n",
    "            sr_subpaths = sr_subpaths.sort_values()  # 升序排序\n",
    "            node_subpath = sr_subpaths.index[-1]  # 最短路径\n",
    "            node_value = sr_subpaths[-1]   # 最短路径对应的值\n",
    "            # 把 node_now 的最短路径添加到 paths 中\n",
    "            paths[node_subpath] = node_value\n",
    "    # 所有层求完后，找出最后一层中各个节点的路径最短的路径\n",
    "    sr_paths = pd.Series(paths)\n",
    "    sr_paths = sr_paths.sort_values()  # 按照升序排序\n",
    "    return sr_paths.index[-1]  # 返回最短路径（概率值最大的路径）\n",
    "\n",
    "\n",
    "def text2ids(text):\n",
    "    \"\"\"把字片段text转为 ids.\"\"\"\n",
    "    words = list(text)\n",
    "    ids = list(word2id[words])\n",
    "    if len(ids) >= max_len:  # 长则弃掉\n",
    "        print u'输出片段超过%d部分无法处理' % (max_len) \n",
    "        return ids[:max_len]\n",
    "    ids.extend([0]*(max_len-len(ids))) # 短则补全\n",
    "    ids = np.asarray(ids).reshape([-1, max_len])\n",
    "    return ids\n",
    "\n",
    "\n",
    "def simple_cut(text):\n",
    "    \"\"\"对一个片段text（标点符号把句子划分为多个片段）进行预测。\"\"\"\n",
    "    if text:\n",
    "        text_len = len(text)\n",
    "        X_batch = text2ids(text)  # 这里每个 batch 是一个样本\n",
    "        fetches = [y_pred]\n",
    "        feed_dict = {X_inputs:X_batch, lr:1.0, batch_size:1, keep_prob:1.0}\n",
    "        _y_pred = sess.run(fetches, feed_dict)[0][:text_len]  # padding填充的部分直接丢弃\n",
    "        nodes = [dict(zip(['s','b','m','e'], each[1:])) for each in _y_pred]\n",
    "        tags = viterbi(nodes)\n",
    "        words = []\n",
    "        for i in range(len(text)):\n",
    "            if tags[i] in ['s', 'b']:\n",
    "                words.append(text[i])\n",
    "            else:\n",
    "                words[-1] += text[i]\n",
    "        return words\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "def cut_word(sentence):\n",
    "    \"\"\"首先将一个sentence根据标点和英文符号/字符串划分成多个片段text，然后对每一个片段分词。\"\"\"\n",
    "    not_cuts = re.compile(u'([0-9\\da-zA-Z ]+)|[。，、？！.\\.\\?,!]')\n",
    "    result = []\n",
    "    start = 0\n",
    "    for seg_sign in not_cuts.finditer(sentence):\n",
    "        result.extend(simple_cut(sentence[start:seg_sign.start()]))\n",
    "        result.append(sentence[seg_sign.start():seg_sign.end()])\n",
    "        start = seg_sign.end()\n",
    "    result.extend(simple_cut(sentence[start:]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 例一\n",
    "sentence = u'人们思考问题往往不是从零开始的。就好像你现在阅读这篇文章一样，你对每个词的理解都会依赖于你前面看到的一些词，\\\n",
    "      而不是把你前面看的内容全部抛弃了，忘记了，再去理解这个单词。也就是说，人们的思维总是会有延续性的。'\n",
    "result = cut_word(sentence)\n",
    "rss = ''\n",
    "for each in result:\n",
    "    rss = rss + each + ' / '\n",
    "print rss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 例二\n",
    "sentence = u'天舟一号是我国自主研制的首艘货运飞船，由于它只运货，不送人，所以被形象地称为太空“快递小哥”。\\\n",
    "    它采用两舱式结构，直径较小的是推进舱，直径较大的为货物舱。其最大直径达到3.35米，飞船全长10.6米，载荷能力达到了6.5吨，\\\n",
    "    满载货物时重13.5吨。如果此次满载的话，它很可能将成为中国发射进入太空的质量最大的有效载荷。甚至比天宫二号空间实验室还大，\\\n",
    "    后者全长10.4米，直径同为3.35米，质量为8.6吨。'\n",
    "result = cut_word(sentence)\n",
    "rss = ''\n",
    "for each in result:\n",
    "    rss = rss + each + ' / '\n",
    "print rss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 例三\n",
    "sentence = u'南京市长江大桥'\n",
    "result = cut_word(sentence)\n",
    "rss = ''\n",
    "for each in result:\n",
    "    rss = rss + each + '/ '\n",
    "print rss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结论：本例子使用 Bi-directional LSTM 来完成了序列标注的问题。本例中展示的是一个分词任务，但是还有其他的序列标注问题都是可以通过这样一个架构来实现的，比如 POS（词性标注）、NER（命名实体识别）等。在本例中，最后的分词效果还不是非常好，但已经达到了实用的水平，而且模型也只是粗略地跑了一遍，还没有进行任何的参数优化。最后的维特比译码中转移概率只是简单的用了等概分布，如果能根据训练语料以统计结果作为概率分布的话相信结果能够进一步提高。<br/>\n",
    "在模型构造中，我们对 Bi-directional LSTM 模型进行了比较详细的展开分析，从而对模型有了深入的理解。这很大程度上也得益于 TensorFlow 比较底层，如果是用 keras 框架的话，虽然只需要短短的几行代码就搞定了，但是我们对于模型的理解估计不会这么深入。"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
