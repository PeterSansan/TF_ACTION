{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 动态RNN\n",
    "实验目的：判断序列是否为线性，即序列二分类问题。序列的长度不是固定的，这个LSTM模型也是为多对1的，即输入为一个序列，输出为一个固定长度的数值，跟前面的用RNN做手写识别的例子类似，不同点是<font color=\"red\">这个例子的输入序列的长度不同</font>，即`timestep`不同，每一个序列按照自身的序列长度 = `timestep`做更新。\n",
    "\n",
    "### 1、生成数据：ToySequenceDate类\n",
    "生成一组长度不等的随机序列，最短序列长度默认为3，最长序列默认为20，最短序列默认为3,其中的数据用0填充；用类函数next可以批量地取数据，得到的分别是`batch_x`,`batch_y`,`batch_seqlen`;\n",
    "如把batch_size = 3;\n",
    "\n",
    "$ batch_x\n",
    "\n",
    "> [[[0.375], [0.417], [0.675], [0.212], [0.799], [0.713], [0.489], [0.181], [0.757], [0.97], [0.755], [0.357], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]], [[0.426], [0.787], [0.323], [0.295], [0.572], [0.696], [0.651], [0.344], [0.156], [0.855], [0.933], [0.505], [0.533], [0.881], [0.055], [0.318], [0.532], [0.196], [0.54], [0.0]]] # 维度为(2,20,1)\n",
    "\n",
    "$ batch_y\n",
    "> [[0.0, 1.0], [0.0, 1.0]]    # 如果是线性序列，分类标签为[1.0,0.0]，如果非线性序列，分类标签为[0.0,1.0]\n",
    "\n",
    "$batch_seqlen\n",
    "\n",
    "> [12, 19]  # 序列有效的真实长度\n",
    "\n",
    "### 2、网络模型：dynamicRNN(x, seqlen, weights, biases):\n",
    "```\n",
    "输入节点：1个\n",
    "LSTM层节点： 64个\n",
    "输出节点：2个\n",
    "```\n",
    "比上一个模型多了seqlen的变量\n",
    "\n",
    "![模型图](http://ogtxggxo6.bkt.clouddn.com/ls.png?imageslim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "A Dynamic Recurrent Neural Network (LSTM) implementation example using\n",
    "TensorFlow library. This example is using a toy dataset to classify linear\n",
    "sequences. The generated sequences have variable length.\n",
    "\n",
    "Long Short Term Memory paper: http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "# ====================\n",
    "#  TOY DATA GENERATOR\n",
    "# ====================\n",
    "class ToySequenceData(object):\n",
    "    \"\"\" Generate sequence of data with dynamic length.\n",
    "    This class generate samples for training:\n",
    "    - Class 0: linear sequences (i.e. [0, 1, 2, 3,...])\n",
    "    - Class 1: random sequences (i.e. [1, 3, 10, 7,...])\n",
    "\n",
    "    NOTICE:\n",
    "    We have to pad each sequence to reach 'max_seq_len' for TensorFlow\n",
    "    consistency (we cannot feed a numpy array with inconsistent\n",
    "    dimensions). The dynamic calculation will then be perform thanks to\n",
    "    'seqlen' attribute that records every actual sequence length.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_samples=1000, max_seq_len=20, min_seq_len=3,\n",
    "                 max_value=1000):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.seqlen = []\n",
    "        for i in range(n_samples):\n",
    "            # Random sequence length\n",
    "            len = random.randint(min_seq_len, max_seq_len)\n",
    "            # Monitor sequence length for TensorFlow dynamic calculation\n",
    "            self.seqlen.append(len)\n",
    "            # Add a random or linear int sequence (50% prob)\n",
    "            if random.random() < .5:\n",
    "                # Generate a linear sequence\n",
    "                rand_start = random.randint(0, max_value - len)\n",
    "                s = [[float(i)/max_value] for i in\n",
    "                     range(rand_start, rand_start + len)]\n",
    "                # Pad sequence for dimension consistency\n",
    "                s += [[0.] for i in range(max_seq_len - len)]\n",
    "                self.data.append(s)\n",
    "                self.labels.append([1., 0.])\n",
    "            else:\n",
    "                # Generate a random sequence\n",
    "                s = [[float(random.randint(0, max_value))/max_value]\n",
    "                     for i in range(len)]\n",
    "                # Pad sequence for dimension consistency\n",
    "                s += [[0.] for i in range(max_seq_len - len)]\n",
    "                self.data.append(s)\n",
    "                self.labels.append([0., 1.])\n",
    "        self.batch_id = 0\n",
    "\n",
    "    def next(self, batch_size):\n",
    "        \"\"\" Return a batch of data. When dataset end is reached, start over.\n",
    "        \"\"\"\n",
    "        if self.batch_id == len(self.data):\n",
    "            self.batch_id = 0\n",
    "        batch_data = (self.data[self.batch_id:min(self.batch_id +\n",
    "                                                  batch_size, len(self.data))])\n",
    "        batch_labels = (self.labels[self.batch_id:min(self.batch_id +\n",
    "                                                  batch_size, len(self.data))])\n",
    "        batch_seqlen = (self.seqlen[self.batch_id:min(self.batch_id +\n",
    "                                                  batch_size, len(self.data))])\n",
    "        self.batch_id = min(self.batch_id + batch_size, len(self.data))\n",
    "        return batch_data, batch_labels, batch_seqlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========\n",
    "#   MODEL\n",
    "# ==========\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "training_iters = 2000000\n",
    "batch_size = 128\n",
    "display_step = 200\n",
    "\n",
    "# Network Parameters\n",
    "seq_max_len = 20 # Sequence max length\n",
    "n_hidden = 64 # hidden layer num of features\n",
    "n_classes = 2 # linear sequence or not\n",
    "\n",
    "trainset = ToySequenceData(n_samples=1000, max_seq_len=seq_max_len)\n",
    "testset = ToySequenceData(n_samples=500, max_seq_len=seq_max_len)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, seq_max_len, 1])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "# A placeholder for indicating each sequence length\n",
    "seqlen = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "\n",
    "def dynamicRNN(x, seqlen, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)  # 原来数据维度(batch_size,n_steps,n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "    \n",
    "    # Unstack to get a list of 'n_steps' tensors of shape (batch_size, n_input) # 重构数据维度(n_steps,batch_size,n_input)\n",
    "    x = tf.unstack(x, seq_max_len, 1)  # 默认n_steps = seq_max_len = 20\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden)\n",
    "\n",
    "    # Get lstm cell output, providing 'sequence_length' will perform dynamic\n",
    "    # calculation.\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cell, x, dtype=tf.float32,\n",
    "                                sequence_length=seqlen)  # 这里就是与最基本的LSTM的区别\n",
    "\n",
    "    # 上一个例子中，如果定义一个seqlen的序列为[28,28,28,...]，结果是一样的\n",
    "    # When performing dynamic calculation, we must retrieve the last\n",
    "    # dynamically computed output, i.e., if a sequence length is 10, we need\n",
    "    # to retrieve the 10th output.\n",
    "    # However TensorFlow doesn't support advanced indexing yet, so we build\n",
    "    # a custom op that for each sample in batch size, get its length and\n",
    "    # get the corresponding relevant output.\n",
    "\n",
    "    # 'outputs' is a list of output at every timestep, we pack them in a Tensor\n",
    "    # and change back dimension to [batch_size, n_step, n_input]\n",
    "    outputs = tf.stack(outputs)  # 影响不大,把list转为tensor\n",
    "    outputs = tf.transpose(outputs, [1, 0, 2])  # 转置函数，本来的outpus维度为(n_step，batch_size,hidden),\n",
    "    # 现在的维度为(batch_size,n_step,hidden)\n",
    "          \n",
    "    # Hack to build the indexing and retrieve the right output.\n",
    "    batch_size = tf.shape(outputs)[0] # 取batch_size\n",
    "    # Start indices for each sample\n",
    "    index = tf.range(0, batch_size) * seq_max_len + (seqlen - 1)\n",
    "    # Indexing\n",
    "    outputs = tf.gather(tf.reshape(outputs, [-1, n_hidden]), index) # 这里的outputs先是由原来的(batch_size,n_step,hidden)\n",
    "    # 转为(batch_size*n_step,hidden),这样的每一列的hidden输出，都包括了多个batch_size,每一个batch_size相差seqlen_max,即20，通过\n",
    "    # 加上seqlen来定位到真正有效的长度，即长度为10，即为10th的隐藏层输出\n",
    "    # Linear activation, using outputs computed above\n",
    "    return tf.matmul(outputs, weights['out']) + biases['out']\n",
    "\n",
    "pred = dynamicRNN(x, seqlen, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 25600, Minibatch Loss= 0.688734, Training Accuracy= 0.53846\n",
      "Iter 51200, Minibatch Loss= 0.686731, Training Accuracy= 0.50962\n",
      "Iter 76800, Minibatch Loss= 0.685059, Training Accuracy= 0.54808\n",
      "Iter 102400, Minibatch Loss= 0.683110, Training Accuracy= 0.57692\n",
      "Iter 128000, Minibatch Loss= 0.680413, Training Accuracy= 0.57692\n",
      "Iter 153600, Minibatch Loss= 0.676176, Training Accuracy= 0.64423\n",
      "Iter 179200, Minibatch Loss= 0.668738, Training Accuracy= 0.70192\n",
      "Iter 204800, Minibatch Loss= 0.653560, Training Accuracy= 0.73077\n",
      "Iter 230400, Minibatch Loss= 0.614810, Training Accuracy= 0.73077\n",
      "Iter 256000, Minibatch Loss= 0.531648, Training Accuracy= 0.78846\n",
      "Iter 281600, Minibatch Loss= 0.467554, Training Accuracy= 0.82692\n",
      "Iter 307200, Minibatch Loss= 0.444708, Training Accuracy= 0.83654\n",
      "Iter 332800, Minibatch Loss= 0.431049, Training Accuracy= 0.82692\n",
      "Iter 358400, Minibatch Loss= 0.420798, Training Accuracy= 0.82692\n",
      "Iter 384000, Minibatch Loss= 0.412461, Training Accuracy= 0.83654\n",
      "Iter 409600, Minibatch Loss= 0.405410, Training Accuracy= 0.84615\n",
      "Iter 435200, Minibatch Loss= 0.399312, Training Accuracy= 0.84615\n",
      "Iter 460800, Minibatch Loss= 0.393962, Training Accuracy= 0.86538\n",
      "Iter 486400, Minibatch Loss= 0.389211, Training Accuracy= 0.86538\n",
      "Iter 512000, Minibatch Loss= 0.384933, Training Accuracy= 0.86538\n",
      "Iter 537600, Minibatch Loss= 0.381007, Training Accuracy= 0.86538\n",
      "Iter 563200, Minibatch Loss= 0.377308, Training Accuracy= 0.86538\n",
      "Iter 588800, Minibatch Loss= 0.373709, Training Accuracy= 0.85577\n",
      "Iter 614400, Minibatch Loss= 0.370084, Training Accuracy= 0.85577\n",
      "Iter 640000, Minibatch Loss= 0.366325, Training Accuracy= 0.85577\n",
      "Iter 665600, Minibatch Loss= 0.362348, Training Accuracy= 0.85577\n",
      "Iter 691200, Minibatch Loss= 0.358108, Training Accuracy= 0.86538\n",
      "Iter 716800, Minibatch Loss= 0.353580, Training Accuracy= 0.86538\n",
      "Iter 742400, Minibatch Loss= 0.348647, Training Accuracy= 0.87500\n",
      "Iter 768000, Minibatch Loss= 0.342883, Training Accuracy= 0.87500\n",
      "Iter 793600, Minibatch Loss= 0.335310, Training Accuracy= 0.87500\n",
      "Iter 819200, Minibatch Loss= 0.323755, Training Accuracy= 0.87500\n",
      "Iter 844800, Minibatch Loss= 0.303415, Training Accuracy= 0.87500\n",
      "Iter 870400, Minibatch Loss= 0.321832, Training Accuracy= 0.83654\n",
      "Iter 896000, Minibatch Loss= 0.385402, Training Accuracy= 0.83654\n",
      "Iter 921600, Minibatch Loss= 0.310174, Training Accuracy= 0.82692\n",
      "Iter 947200, Minibatch Loss= 0.156201, Training Accuracy= 0.94231\n",
      "Iter 972800, Minibatch Loss= 0.120676, Training Accuracy= 0.94231\n",
      "Iter 998400, Minibatch Loss= 0.107740, Training Accuracy= 0.95192\n",
      "Iter 1024000, Minibatch Loss= 0.099711, Training Accuracy= 0.95192\n",
      "Iter 1049600, Minibatch Loss= 0.094423, Training Accuracy= 0.95192\n",
      "Iter 1075200, Minibatch Loss= 0.090499, Training Accuracy= 0.96154\n",
      "Iter 1100800, Minibatch Loss= 0.087343, Training Accuracy= 0.96154\n",
      "Iter 1126400, Minibatch Loss= 0.084676, Training Accuracy= 0.96154\n",
      "Iter 1152000, Minibatch Loss= 0.082345, Training Accuracy= 0.96154\n",
      "Iter 1177600, Minibatch Loss= 0.080291, Training Accuracy= 0.96154\n",
      "Iter 1203200, Minibatch Loss= 0.078469, Training Accuracy= 0.96154\n",
      "Iter 1228800, Minibatch Loss= 0.076829, Training Accuracy= 0.96154\n",
      "Iter 1254400, Minibatch Loss= 0.075338, Training Accuracy= 0.96154\n",
      "Iter 1280000, Minibatch Loss= 0.073975, Training Accuracy= 0.96154\n",
      "Iter 1305600, Minibatch Loss= 0.072726, Training Accuracy= 0.96154\n",
      "Iter 1331200, Minibatch Loss= 0.071631, Training Accuracy= 0.97115\n",
      "Iter 1356800, Minibatch Loss= 0.070625, Training Accuracy= 0.97115\n",
      "Iter 1382400, Minibatch Loss= 0.069588, Training Accuracy= 0.97115\n",
      "Iter 1408000, Minibatch Loss= 0.068581, Training Accuracy= 0.97115\n",
      "Iter 1433600, Minibatch Loss= 0.067625, Training Accuracy= 0.97115\n",
      "Iter 1459200, Minibatch Loss= 0.066732, Training Accuracy= 0.97115\n",
      "Iter 1484800, Minibatch Loss= 0.065885, Training Accuracy= 0.97115\n",
      "Iter 1510400, Minibatch Loss= 0.065097, Training Accuracy= 0.97115\n",
      "Iter 1536000, Minibatch Loss= 0.064368, Training Accuracy= 0.97115\n",
      "Iter 1561600, Minibatch Loss= 0.063684, Training Accuracy= 0.97115\n",
      "Iter 1587200, Minibatch Loss= 0.063004, Training Accuracy= 0.97115\n",
      "Iter 1612800, Minibatch Loss= 0.062287, Training Accuracy= 0.97115\n",
      "Iter 1638400, Minibatch Loss= 0.061551, Training Accuracy= 0.97115\n",
      "Iter 1664000, Minibatch Loss= 0.060837, Training Accuracy= 0.97115\n",
      "Iter 1689600, Minibatch Loss= 0.060158, Training Accuracy= 0.97115\n",
      "Iter 1715200, Minibatch Loss= 0.059516, Training Accuracy= 0.97115\n",
      "Iter 1740800, Minibatch Loss= 0.058910, Training Accuracy= 0.97115\n",
      "Iter 1766400, Minibatch Loss= 0.058337, Training Accuracy= 0.97115\n",
      "Iter 1792000, Minibatch Loss= 0.057795, Training Accuracy= 0.97115\n",
      "Iter 1817600, Minibatch Loss= 0.057282, Training Accuracy= 0.97115\n",
      "Iter 1843200, Minibatch Loss= 0.056794, Training Accuracy= 0.97115\n",
      "Iter 1868800, Minibatch Loss= 0.056329, Training Accuracy= 0.97115\n",
      "Iter 1894400, Minibatch Loss= 0.055886, Training Accuracy= 0.97115\n",
      "Iter 1920000, Minibatch Loss= 0.055462, Training Accuracy= 0.97115\n",
      "Iter 1945600, Minibatch Loss= 0.055056, Training Accuracy= 0.97115\n",
      "Iter 1971200, Minibatch Loss= 0.054666, Training Accuracy= 0.97115\n",
      "Iter 1996800, Minibatch Loss= 0.054291, Training Accuracy= 0.97115\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.976\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x, batch_y, batch_seqlen = trainset.next(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y,\n",
    "                                       seqlen: batch_seqlen})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y,\n",
    "                                                seqlen: batch_seqlen})\n",
    "            # Calculate batch loss\n",
    "            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y,\n",
    "                                             seqlen: batch_seqlen})\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy\n",
    "    test_data = testset.data\n",
    "    test_label = testset.labels\n",
    "    test_seqlen = testset.seqlen\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_data, y: test_label,\n",
    "                                      seqlen: test_seqlen}))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
